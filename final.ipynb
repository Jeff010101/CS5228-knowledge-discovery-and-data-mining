{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from IPython.display import clear_output\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report,accuracy_score,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier,plot_importance\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, StandardScaler\n",
    "\n",
    "# sampler objects\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, SVMSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas_profiling as pp\n",
    "import itertools\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "#%% ------path\n",
    "data_path = ''\n",
    "data_path_train = os.path.join(data_path,'train/train')\n",
    "file = os.path.join(data_path,'train_kaggle.csv')\n",
    "pro_file = os.path.join(data_path,'train_kaggle_pro.csv')\n",
    "test_pro_file = os.path.join(data_path,'test_kaggle_pro.csv')\n",
    "data_path_test = os.path.join(data_path,'test/test')\n",
    "\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = []\n",
    "\n",
    "s = len(df)\n",
    "for i in range(0,s):\n",
    "    print(\"Loading in progess... %.1f%%\" % (i/s * 100.0))\n",
    "    data_raw.append(np.load(os.path.join(data_path_train,str(i)+'.npy')))\n",
    "    clear_output()\n",
    "print(len(data_raw), \" data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pre processing of Training data\n",
    "\n",
    "# padding each data to length of 336, because the longest data length for 1 and 0 are both 336\n",
    "f_n = 336 \n",
    "# calculate min/max/var value for below features\n",
    "mm = [1,3,5,7,8,9,11,12,13,14,15,17,18,19,20,21,22,23,24,26,27,28,29,30,31,32,35,36,37,38,39] \n",
    "doc = open(pro_file,'w',encoding='utf-8')\n",
    "\n",
    "string = \",\".join(map(str, [i for i in range(1, 39*f_n+2+len(mm)*3)])) + ',label\\n'\n",
    "doc.write(string)\n",
    "\n",
    "s = len(data_raw)\n",
    "for i in range(s):\n",
    "    print(\"Processing in progess... %.1f%%\" % (i/s * 100.0))\n",
    "    label = df.iloc[i]['label']\n",
    "    feature = data_raw[i]\n",
    "    m = []\n",
    "    for j in mm:\n",
    "        m = np.concatenate((m, np.nanmin(feature.T[j]), \n",
    "                            np.nanmax(feature.T[j]), \n",
    "                            np.nanvar(feature.T[j])), axis=None)\n",
    "    m = np.concatenate((m, np.nanmax(feature.T[2])), axis=None)\n",
    "    feature = np.delete(feature, 2, axis=1)  \n",
    "    # normalize data\n",
    "    feature = scaler.fit_transform(feature)\n",
    "    # padding\n",
    "    if(len(feature) < f_n):\n",
    "        feature = np.vstack([feature, np.full((f_n - len(feature), 39), np.nan)])\n",
    "    f_d = pd.DataFrame(feature)\n",
    "    if(len(f_d) > f_n):\n",
    "        f_d = f_d[:f_n]\n",
    "    # linear interpolate\n",
    "    f_d.interpolate(method='linear', limit_direction='forward', axis=0, inplace=True)\n",
    "    l = []\n",
    "    f_d = f_d.T\n",
    "    string = ''\n",
    "    for j, k in f_d.iterrows():\n",
    "        l.append(list(k))\n",
    "    l.append(list(m))\n",
    "    merged = list(itertools.chain.from_iterable(l))\n",
    "    string = \",\".join(map(str,merged)) + \",\" + str(label) + '\\n'\n",
    "    doc.write(string)\n",
    "    clear_output()\n",
    "doc.close()\n",
    "\n",
    "data = pd.read_csv(pro_file)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre processing of Test data\n",
    "\n",
    "f_n = 336\n",
    "mm = [1,3,5,7,8,9,11,12,13,14,15,17,18,19,20,21,22,23,24,26,27,28,29,30,31,32,35,36,37,38,39]\n",
    "doc = open(test_pro_file,'w',encoding='utf-8')\n",
    "\n",
    "string = \",\".join(map(str, [i for i in range(1, 39*f_n+2+len(mm)*3)])) + '\\n'\n",
    "doc.write(string)\n",
    "\n",
    "for i in range(0,10000):\n",
    "    print(\"Processing in progess... %.1f%%\" % (i/10000 * 100.0))\n",
    "    feature = np.load(os.path.join(data_path_test,str(i)+'.npy'))\n",
    "    string = '\\n'\n",
    "    m = []\n",
    "    for j in mm:\n",
    "        m = np.concatenate((m, np.nanmin(feature.T[j]), \n",
    "                            np.nanmax(feature.T[j]), \n",
    "                            np.nanvar(feature.T[j])), axis=None)\n",
    "    m = np.concatenate((m, np.nanmax(feature.T[2])), axis=None)\n",
    "    feature = np.delete(feature, 2, axis=1)  \n",
    "    feature = min_max_scaler.fit_transform(feature)\n",
    "    if(len(feature) < f_n):\n",
    "        feature = np.vstack([feature, np.full((f_n - len(feature), 39), np.nan)])\n",
    "    f_d = pd.DataFrame(feature)\n",
    "    if(len(f_d) > f_n):\n",
    "        f_d = f_d[:f_n]\n",
    "    f_d.interpolate(method='linear', limit_direction='forward', axis=0, inplace=True)\n",
    "    l = []\n",
    "    f_d = f_d.T\n",
    "    for j, k in f_d.iterrows():\n",
    "        l.append(list(k))\n",
    "    l.append(list(m))\n",
    "    merged = list(itertools.chain.from_iterable(l))\n",
    "    string = string + \",\".join(map(str,merged))\n",
    "    doc.write(string)\n",
    "    clear_output()\n",
    "doc.close()\n",
    "\n",
    "test_data = pd.read_csv(test_pro_file)\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 3000 features for training\n",
    "embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=3000)\n",
    "embeded_rf_selector.fit(X, y)\n",
    "\n",
    "embeded_rf_support = embeded_rf_selector.get_support()\n",
    "embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into traning set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[['label']]\n",
    "X = data.drop(columns=['label'])\n",
    "\n",
    "X = X[embeded_rf_feature]\n",
    "\n",
    "X.fillna(-1, inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over/Under sample data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_data(ratio = 0.5):\n",
    "    data = pd.read_csv(\"train_kaggle_pro.csv\")\n",
    "\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    y_test = test[['label']]\n",
    "    x_test = test.drop(columns=['label'])\n",
    "    \n",
    "    y_train = train[['label']]\n",
    "    x_train = train.drop(columns=['label'])\n",
    "    \n",
    "    x_test.fillna(-1, inplace=True)\n",
    "    x_train.fillna(-1, inplace=True)\n",
    "    X_train_oversample, y_train_oversample = SMOTE(sampling_strategy=ratio).fit_resample(x_train, y_train)    \n",
    "    \n",
    "    ttt = pd.DataFrame(y_train_oversample)\n",
    "    print(\"Percentage of 0: \", len(ttt[ttt[0] == 0])/len(ttt))\n",
    "    print(\"Percentage of 1: \", len(ttt[ttt[0] == 1])/len(ttt))\n",
    "    print(\"Total number of resampled data: \", len(ttt))\n",
    "    X_train_oversample = pd.DataFrame(X_train_oversample)\n",
    "    X_train_oversample.columns = X_test.columns\n",
    "    return X_train_oversample, x_test, pd.DataFrame(y_train_oversample), y_test\n",
    "\n",
    "def oversample_resampled_data(X, y, ratio = 0.5):\n",
    "    X.fillna(-1, inplace=True)\n",
    "    X_resampled, y_resampled = SMOTE(sampling_strategy=ratio).fit_resample(X, y)\n",
    "    \n",
    "    ttt = pd.DataFrame(y_resampled)\n",
    "    print(\"Percentage of 0: \", len(ttt[ttt[0] == 0])/len(ttt))\n",
    "    print(\"Percentage of 1: \", len(ttt[ttt[0] == 1])/len(ttt))\n",
    "    print(\"Total number of resampled data: \", len(ttt))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def resampled_data():\n",
    "    rus = RandomUnderSampler(random_state=42, sampling_strategy=0.2)\n",
    "    X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def umdersample_data():\n",
    "    X_resampled, y_resampled = resampled_data()\n",
    "\n",
    "    ttt = pd.DataFrame(y_resampled)\n",
    "    print(\"Percentage of 0: \", len(ttt[ttt[0] == 0])/len(ttt))\n",
    "    print(\"Percentage of 1: \", len(ttt[ttt[0] == 1])/len(ttt))\n",
    "    print(\"Total number of resampled data: \", len(ttt))\n",
    "    \n",
    "    X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_resampled,y_resampled,test_size = 0.3,random_state = 0)    \n",
    "    return X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning Model - LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = [(X_test, y_test)]\n",
    "clf = lgb.LGBMRegressor(boosting_type=\"goss\", \n",
    "                        num_iterations = 145, \n",
    "                        objective='binary',\n",
    "                        metric='auc',\n",
    "                        learning_rate=0.09,\n",
    "                        num_threads=2,\n",
    "                        is_unbalance=True)\n",
    "clf.fit(X_train,y_train, eval_set=eval_set, eval_metric=\"auc\", verbose=1, early_stopping_rounds=20)\n",
    "y_pred_prob = clf.predict(X_test)\n",
    "y_pred = np.where(y_pred_prob > 0.5, 1, 0)\n",
    "recall_acc = recall_score(y_test,y_pred)\n",
    "print(\"Recall: %.2f%%\" % (recall_acc * 100.0))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    "    'max_depth': [4,5,6,7,8,9],\n",
    " 'n_estimators': [200,300,400]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = clf, param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(X_train,y_train)\n",
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train_kaggle_pro.csv\")\n",
    "\n",
    "y = data[['label']]\n",
    "X = data.drop(columns=['label'])\n",
    "X = X[embeded_rf_feature]\n",
    "X.fillna(-1, inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_oversample, y_train_oversample = SMOTE(sampling_strategy=0.2).fit_resample(X, y) \n",
    "clf_pre = lgb.LGBMRegressor(boosting_type=\"goss\", \n",
    "                            num_iterations = 150,\n",
    "                            objective='binary',\n",
    "                            metric='auc',\n",
    "                            num_threads=2,\n",
    "                            learning_rate=0.09,\n",
    "                            is_unbalance=True)\n",
    "clf_pre.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test_kaggle_pro.csv\")\n",
    "test_data.fillna(-1, inplace=True)\n",
    "test_data = test_data[embeded_rf_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(test_data)\n",
    "df_y = pd.DataFrame(y_test_pred, columns=[\"label\"])\n",
    "idx = pd.Series(range(0, len(df_y)))\n",
    "upload = pd.concat([idx, df_y], axis=1)\n",
    "\n",
    "upload.columns=['Id','Predicted']\n",
    "upload.to_csv('submiss.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
